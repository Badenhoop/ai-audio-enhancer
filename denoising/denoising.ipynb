{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/philipp/ai-audio-enhancer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio as T\n",
    "import torchaudio.transforms as TT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from moviepy.editor import *\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "from dataset import build_dataloader\n",
    "from model import DiffusionUNetModel, DiffusionEmbedding\n",
    "from config import Config\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert mp4 to mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp4_to_aac(in_path, out_path):\n",
    "    subprocess.run(\n",
    "        ['ffmpeg', '-y', '-i', in_path, '-c', 'copy', out_path], \n",
    "        check=True, \n",
    "        stdout=subprocess.DEVNULL, \n",
    "        stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_receptive_field import receptive_field\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    ")\n",
    "\n",
    "size = 44100 * 5\n",
    "receptive_field(model.cuda(), input_size=(1, size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receptive_field(kernel_size, num_layers, dilation_cycle):\n",
    "    return (kernel_size - 1) * sum(dilation_cycle[i % len(dilation_cycle)] for i in range(num_layers)) + 1\n",
    "\n",
    "receptive_field(7, 30, [2**i for i in range(0, 9+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion Models:\n",
    "x -> x1, x2, xT ~ N(0, I)\n",
    "Train to reverse noise: x(t) -> x(t-1)\n",
    "\n",
    "Idea:\n",
    "Given input clip x, instead of reversing the process directly, add noise to it and then try to reverse the noise.\n",
    "Hypothesis: The output will not sound like the original but instead more like an improved version of the original but of course more different.\n",
    "When we add a lot of noise, the output will sound completely different than the original. \n",
    "In order to keep the original information, use features from another encoder (for example trained on contrasting cover songs).\n",
    "\n",
    "Diffusion Model TODOs:\n",
    "- Dataset\n",
    "- DataLoader\n",
    "- Colate function\n",
    "- Model\n",
    "- Training loop\n",
    "- Inference\n",
    "\n",
    "Memory consumption:\n",
    "- perform checkpointing\n",
    "\n",
    "Prototype Requirements:\n",
    "    - Denoise 5s segments\n",
    "    - Sampling rate: 44100\n",
    "\n",
    "Model architecture:\n",
    "- DiffWave:\n",
    "    - Each layer has the full output resolution\n",
    "    - Uses exponential dilation factors to have a receptive field that spans the entire input\n",
    "    - Problem: consumes a lot of memory\n",
    "    - Possible solution: trade compute for memory by using checkpointing -> too slow\n",
    "- U-Net WaveNet:\n",
    "    - Downsamples the sequence to reduce memory footprint and increase performance\n",
    "    - Problem: the ear is very sensitive to errors in the high frequencies which are troublesome during the upscaling operations\n",
    "-> Final decision: \n",
    "    - Use U-Net because WaveNet is either way too memory demanding or way too slow when using checkpointing\n",
    "    - Also, we can try to optimize the hell out of the U-Net architecture (skip connections, attention, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df = pd.read_csv('scraper/songs/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = songs_df.sample(1000)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'scraper/songs'\n",
    "for row in tqdm(list(sample.itertuples())):\n",
    "    src = os.path.join(root, row.path)\n",
    "    dst = f'denoising/data/{row.id}.mp4'\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = build_dataloader(\n",
    "    directory='denoising/data',\n",
    "    audio_format='mp4',\n",
    "    batch_size=8,\n",
    "    audio_length=5 * 44100,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0041, -0.0708, -0.0289,  ...,  0.0068, -0.0589,  0.0546],\n",
      "        [ 0.3062,  0.3439,  0.3755,  ...,  0.2264, -0.2304,  0.3136],\n",
      "        [ 0.1016,  0.0852,  0.0952,  ...,  0.3542,  0.2962,  0.3777],\n",
      "        ...,\n",
      "        [ 0.0127,  0.0119,  0.0119,  ...,  0.0449,  0.0449,  0.0479],\n",
      "        [ 0.0644,  0.1693,  0.1052,  ...,  0.1355,  0.2810,  0.0739],\n",
      "        [ 0.1137,  0.1390,  0.1248,  ...,  0.0182,  0.0182, -0.0102]])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.fromfile('denoising/configs/unet.py')\n",
    "config.dump('tmp/config.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_embedding = DiffusionEmbedding(\n",
    "            num_diffusion_steps=50,\n",
    "            num_channels=32)\n",
    "embedding = diffusion_embedding(torch.tensor([0, 1, 2]))\n",
    "embedding.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20f86334d7d7ff0604c435ec71991a016a149996346b83125c95b441679e69a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
