{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/philipp/ai-audio-enhancer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/anaconda3/envs/audio/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio as T\n",
    "import torchaudio.transforms as TT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from moviepy.editor import *\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert mp4 to mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/home/philipp/anaconda3/envs/audio --cc=/opt/conda/conda-bld/ffmpeg_1597178665428/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'tmp/test.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : dash\n",
      "    minor_version   : 0\n",
      "    compatible_brands: iso6mp41\n",
      "    creation_time   : 2021-09-27T12:36:43.000000Z\n",
      "  Duration: 00:03:10.31, start: 0.000000, bitrate: 48 kb/s\n",
      "    Stream #0:0(eng): Audio: aac (HE-AAC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 2 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2021-09-27T12:36:43.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "Output #0, adts, to 'tmp/test.aac':\n",
      "  Metadata:\n",
      "    major_brand     : dash\n",
      "    minor_version   : 0\n",
      "    compatible_brands: iso6mp41\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0(eng): Audio: aac (HE-AAC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 2 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2021-09-27T12:36:43.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "size=    1143kB time=00:03:10.26 bitrate=  49.2kbits/s speed=1.75e+04x    \n",
      "video:0kB audio:1115kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 2.512208%\n"
     ]
    }
   ],
   "source": [
    "def convert(in_path, out_path):\n",
    "    os.system(f'ffmpeg -i {in_path} -c copy {out_path}')\n",
    "\n",
    "convert('tmp/test.mp4', 'tmp/test.aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sr = librosa.load('tmp/test.aac', mono=True)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_receptive_field import receptive_field\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    ")\n",
    "\n",
    "size = 44100 * 5\n",
    "receptive_field(model.cuda(), input_size=(1, size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receptive_field(kernel_size, num_layers, dilation_cycle):\n",
    "    return (kernel_size - 1) * sum(dilation_cycle[i % len(dilation_cycle)] for i in range(num_layers)) + 1\n",
    "\n",
    "receptive_field(7, 30, [2**i for i in range(0, 9+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion Models:\n",
    "x -> x1, x2, xT ~ N(0, I)\n",
    "Train to reverse noise: x(t) -> x(t-1)\n",
    "\n",
    "Idea:\n",
    "Given input clip x, instead of reversing the process directly, add noise to it and then try to reverse the noise.\n",
    "Hypothesis: The output will not sound like the original but instead more like an improved version of the original but of course more different.\n",
    "When we add a lot of noise, the output will sound completely different than the original. \n",
    "In order to keep the original information, use features from another encoder (for example trained on contrasting cover songs).\n",
    "\n",
    "Diffusion Model TODOs:\n",
    "- Dataset\n",
    "- DataLoader\n",
    "- Colate function\n",
    "- Model\n",
    "- Training loop\n",
    "- Inference\n",
    "\n",
    "Memory consumption:\n",
    "- perform checkpointing\n",
    "\n",
    "Prototype Requirements:\n",
    "    - Denoise 5s segments\n",
    "    - Sampling rate: 44100\n",
    "\n",
    "Model architecture:\n",
    "- DiffWave:\n",
    "    - Each layer has the full output resolution\n",
    "    - Uses exponential dilation factors to have a receptive field that spans the entire input\n",
    "    - Problem: consumes a lot of memory\n",
    "    - Possible solution: trade compute for memory by using checkpointing -> too slow\n",
    "- U-Net WaveNet:\n",
    "    - Downsamples the sequence to reduce memory footprint and increase performance\n",
    "    - Problem: the ear is very sensitive to errors in the high frequencies which are troublesome during the upscaling operations\n",
    "-> Final decision: \n",
    "    - Use U-Net because WaveNet is either way too memory demanding or way too slow when using checkpointing\n",
    "    - Also, we can try to optimize the hell out of the U-Net architecture (skip connections, attention, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, num_diffusion_steps, num_channels):\n",
    "        super().__init__()\n",
    "        self.dim_encoding = int(4 * np.ceil(np.log(num_diffusion_steps)))\n",
    "        self.dim_embedding = num_channels\n",
    "        self.register_buffer('encoding', self._build_encoding(num_diffusion_steps), persistent=False)\n",
    "        self.projection1 = nn.Linear(self.dim_encoding, self.dim_embedding)\n",
    "        self.projection2 = nn.Linear(self.dim_embedding, self.dim_embedding)\n",
    "\n",
    "    def forward(self, diffusion_step):\n",
    "        if diffusion_step.dtype in [torch.int32, torch.int64]:\n",
    "            x = self.encoding[diffusion_step]\n",
    "        else:\n",
    "            x = self._lerp_encoding(diffusion_step)\n",
    "        x = F.leaky_relu(self.projection1(x)) + x\n",
    "        x = F.leaky_relu(self.projection2(x)) + x\n",
    "        return x\n",
    "\n",
    "    def _lerp_encoding(self, t):\n",
    "        low_idx = torch.floor(t).long()\n",
    "        high_idx = torch.ceil(t).long()\n",
    "        low = self.encoding[low_idx]\n",
    "        high = self.encoding[high_idx]\n",
    "        return low + (high - low) * (t - low_idx)\n",
    "\n",
    "    def _build_encoding(self, num_diffusion_steps):\n",
    "        steps = torch.arange(num_diffusion_steps).unsqueeze(1)\n",
    "        dims = torch.arange(self.dim_encoding).unsqueeze(0)\n",
    "        encoding = 2. * np.pi * steps * np.exp(-np.log(2) * dims)\n",
    "        encoding = torch.cat([torch.sin(encoding), torch.cos(encoding)], dim=1)\n",
    "        return encoding\n",
    "\n",
    "\n",
    "class DownsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=1, padding=3),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=7, stride=1, padding=6, dilation=2),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = F.upsample(x, size=skip.shape[2], mode='linear')\n",
    "        x = torch.cat((x, skip), dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, num_diffusion_steps):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.diffusion_embedding = DiffusionEmbedding(\n",
    "            num_diffusion_steps=num_diffusion_steps,\n",
    "            num_channels=32)\n",
    "        self.in_proj = nn.Conv1d(1, 64, 1)\n",
    "        self.out_proj = nn.Conv1d(64, 1, 1)\n",
    "        self.down = nn.ModuleList([\n",
    "            DownsamplingBlock(64, 64),\n",
    "            DownsamplingBlock(64, 64),\n",
    "            DownsamplingBlock(64, 64),\n",
    "            DownsamplingBlock(64, 64),\n",
    "            DownsamplingBlock(64, 128),\n",
    "            DownsamplingBlock(128, 128),\n",
    "            DownsamplingBlock(128, 128),\n",
    "            DownsamplingBlock(128, 128),\n",
    "            DownsamplingBlock(128, 256),\n",
    "            DownsamplingBlock(128, 256),\n",
    "            DownsamplingBlock(256, 256),\n",
    "            DownsamplingBlock(256, 256),\n",
    "        ])\n",
    "        self.up = nn.ModuleList([\n",
    "            UpsamplingBlock(256, 256),\n",
    "            UpsamplingBlock(256, 256),\n",
    "            UpsamplingBlock(256, 256),\n",
    "            UpsamplingBlock(256, 128),\n",
    "            UpsamplingBlock(128, 128),\n",
    "            UpsamplingBlock(128, 128),\n",
    "            UpsamplingBlock(128, 128),\n",
    "            UpsamplingBlock(128, 64),\n",
    "            UpsamplingBlock(64, 64),\n",
    "            UpsamplingBlock(64, 64),\n",
    "            UpsamplingBlock(64, 64),\n",
    "            UpsamplingBlock(64, 64),\n",
    "        ])\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight.data, 1)\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "    \n",
    "    def forward(self, audio, diffusion_step):\n",
    "        embedding = self.diffusion_embedding(diffusion_step)\n",
    "        x = audio.unsqueeze(1) # channel dimension\n",
    "        x = self.in_proj(x)\n",
    "\n",
    "        skip_connections = []\n",
    "        for layer in self.down:\n",
    "            embedding_padded = torch.zeros(x.shape[1], device=x.device)\n",
    "            embedding_padded[:len(embedding)] = embedding\n",
    "            embedding_padded = embedding_padded.view(1, -1, 1)\n",
    "            x = x + embedding_padded\n",
    "            x = layer(x)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        for i, layer in enumerate(self.up):\n",
    "            skip = skip_connections[len(skip_connections) - i - 1]\n",
    "            x = layer(x, skip)\n",
    "\n",
    "        x = self.out_proj(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiffusionModel' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000010?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m DiffusionModel(num_diffusion_steps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39msum\u001b[39m([param\u001b[39m.\u001b[39mnelement() \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters()])\n",
      "\u001b[1;32m/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb Cell 9'\u001b[0m in \u001b[0;36mDiffusionModel.__init__\u001b[0;34m(self, num_diffusion_steps)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=70'>71</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=71'>72</a>\u001b[0m     DownsamplingBlock(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=72'>73</a>\u001b[0m     DownsamplingBlock(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=82'>83</a>\u001b[0m     DownsamplingBlock(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=83'>84</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=84'>85</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=85'>86</a>\u001b[0m     UpsamplingBlock(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=86'>87</a>\u001b[0m     UpsamplingBlock(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=96'>97</a>\u001b[0m     UpsamplingBlock(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=97'>98</a>\u001b[0m ])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=98'>99</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_weights()\n",
      "\u001b[1;32m/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb Cell 9'\u001b[0m in \u001b[0;36mDiffusionModel.init_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=102'>103</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, nn\u001b[39m.\u001b[39mConv2d):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=103'>104</a>\u001b[0m     nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mkaiming_uniform_(m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=104'>105</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(m\u001b[39m.\u001b[39;49mweight, nn\u001b[39m.\u001b[39mLinear):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=105'>106</a>\u001b[0m     nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mxavier_uniform_(m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/philipp/ai-audio-enhancer/denoising/denoising.ipynb#ch0000008?line=106'>107</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(m\u001b[39m.\u001b[39mweight, nn\u001b[39m.\u001b[39mBatchNorm1d):\n",
      "File \u001b[0;32m~/anaconda3/envs/audio/lib/python3.9/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DiffusionModel' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "model = DiffusionModel(num_diffusion_steps=10).cuda()\n",
    "sum([param.nelement() for param in model.parameters()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20f86334d7d7ff0604c435ec71991a016a149996346b83125c95b441679e69a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
